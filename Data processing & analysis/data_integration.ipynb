{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cc19d0-a857-4728-9447-4343843bfed4",
   "metadata": {},
   "source": [
    "# REDDIT \n",
    "## PURPOSE : merging the Reddit and Discord datasets into a unified dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf19c67-8c3e-4430-9b82-786691891ebe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4dd532-d10d-4907-bcc5-636bc74eabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4a0da-32f2-47fb-9bd4-f81f2c41fdea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c7efd8-9297-4143-bb34-234bbbea009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit Dataset:\n",
      "                                               Title     Content  Upvotes  \\\n",
      "0  Cisco Confirms Authenticity of Data After Seco...  No Content        2   \n",
      "1  Mining old data from NASAâ€™s Voyager 2 solves s...  No Content       14   \n",
      "2  Israel built an â€˜AI factoryâ€™ for war. It unlea...  No Content       33   \n",
      "3  Developer fires entire team for AI, now ends u...  No Content      629   \n",
      "4  FDA Approves First Generic of Once-Daily GLP-1...  No Content      157   \n",
      "\n",
      "   Comments_Count                                       Top_Comments  \\\n",
      "0               1  ['Damn ATT, Verizon, Cisco, social security, a...   \n",
      "1               3  ['Â« NASAâ€™s Voyager 2 flyby of Uranus decades a...   \n",
      "2               6  ['AI in war raises concerns.', 'When it comes ...   \n",
      "3              68  ['Creativity and problem-solving abilities, de...   \n",
      "4               9  ['This is generic victoza, which has been out ...   \n",
      "\n",
      "               Created              Author  \\\n",
      "0  2024-12-30 23:15:22          lurker_bee   \n",
      "1  2024-12-30 21:23:56              fchung   \n",
      "2  2024-12-30 20:23:18         MetaKnowing   \n",
      "3  2024-12-30 19:54:32  No-Information6622   \n",
      "4  2024-12-30 18:41:36          Peter55667   \n",
      "\n",
      "                                                 URL  \n",
      "0  https://www.securityweek.com/cisco-confirms-au...  \n",
      "1  https://www.jpl.nasa.gov/news/mining-old-data-...  \n",
      "2  https://www.washingtonpost.com/technology/2024...  \n",
      "3  https://content.techgig.com/technology/develop...  \n",
      "4  https://www.fda.gov/news-events/press-announce...  \n",
      "\n",
      "Discord Dataset:\n",
      "                                              author  \\\n",
      "0  {'id': '244847651013656577', 'username': 'adia...   \n",
      "1  {'id': '345205383373258753', 'username': 'wwol...   \n",
      "2  {'id': '244847651013656577', 'username': 'adia...   \n",
      "3  {'id': '345205383373258753', 'username': 'wwol...   \n",
      "4  {'id': '244847651013656577', 'username': 'adia...   \n",
      "\n",
      "                                             content  \\\n",
      "0  *The mother of learning, is repetition.*\\n*The...   \n",
      "1                                                ðŸ¤¨ ?   \n",
      "2  Only two things can do that. <a:FP_StarSparkle...   \n",
      "3  Fair enough. Will this take me from \"zero to h...   \n",
      "4                           It's your time to waste.   \n",
      "\n",
      "                          timestamp  \n",
      "0  2024-12-30T17:32:53.427000+00:00  \n",
      "1  2024-12-30T17:32:10.251000+00:00  \n",
      "2  2024-12-30T17:31:35.458000+00:00  \n",
      "3  2024-12-30T17:30:50.323000+00:00  \n",
      "4  2024-12-30T17:30:03.453000+00:00  \n",
      "\n",
      "Reddit Columns: ['Title', 'Content', 'Upvotes', 'Comments_Count', 'Top_Comments', 'Created', 'Author', 'URL']\n",
      "Discord Columns: ['author', 'content', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "reddit_data = pd.read_csv(\"reddit_raw_data.csv\")  \n",
    "discord_data = pd.read_csv(\"discord_messages.csv\")  \n",
    "\n",
    "# Preview the datasets\n",
    "print(\"Reddit Dataset:\")\n",
    "print(reddit_data.head())\n",
    "print(\"\\nDiscord Dataset:\")\n",
    "print(discord_data.head())\n",
    "\n",
    "# Check column names for each dataset\n",
    "print(\"\\nReddit Columns:\", reddit_data.columns.tolist())\n",
    "print(\"Discord Columns:\", discord_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b6425-9dcd-430d-8509-d4c43dcf3283",
   "metadata": {},
   "source": [
    "# Discord data Issues Handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ead0a2-5735-43c6-bd90-69f09871fb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types in the 'Author' column:\n",
      "author\n",
      "<class 'str'>    13320\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the data type of the Author column\n",
    "author_column_type = discord_data['author'].apply(type).value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(\"Data types in the 'Author' column:\")\n",
    "print(author_column_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908c583c-4d11-4d3d-8878-7e65402270bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  *The mother of learning, is repetition.*\\n*The...   \n",
      "1                                                ðŸ¤¨ ?   \n",
      "2  Only two things can do that. <a:FP_StarSparkle...   \n",
      "3  Fair enough. Will this take me from \"zero to h...   \n",
      "4                           It's your time to waste.   \n",
      "\n",
      "                          timestamp           Author_ID Author_Username  \\\n",
      "0  2024-12-30T17:32:53.427000+00:00  244847651013656577        adiablue   \n",
      "1  2024-12-30T17:32:10.251000+00:00  345205383373258753      wwolverine   \n",
      "2  2024-12-30T17:31:35.458000+00:00  244847651013656577        adiablue   \n",
      "3  2024-12-30T17:30:50.323000+00:00  345205383373258753      wwolverine   \n",
      "4  2024-12-30T17:30:03.453000+00:00  244847651013656577        adiablue   \n",
      "\n",
      "  Author_Discriminator Author_Global_Name Author_Primary_Guild  \n",
      "0                    0           The Blue                 None  \n",
      "1                    0          Wolverine                 None  \n",
      "2                    0           The Blue                 None  \n",
      "3                    0          Wolverine                 None  \n",
      "4                    0           The Blue                 None  \n"
     ]
    }
   ],
   "source": [
    "import ast \n",
    "\n",
    "# Function to extract relevant fields from the 'Author' column\n",
    "def extract_author_info(author_data):\n",
    "    try:\n",
    "        # Safely evaluate the string representation of the dictionary\n",
    "        author_dict = ast.literal_eval(author_data)\n",
    "        \n",
    "        # Extract the relevant fields\n",
    "        return {\n",
    "            'Author_ID': author_dict.get('id', 'Unknown'),\n",
    "            'Author_Username': author_dict.get('username', 'Unknown'),\n",
    "            'Author_Discriminator': author_dict.get('discriminator', 'Unknown'),\n",
    "            'Author_Global_Name': author_dict.get('global_name', 'Unknown'),\n",
    "            'Author_Primary_Guild': author_dict.get('primary_guild', 'Unknown')\n",
    "        }\n",
    "    except (ValueError, SyntaxError):\n",
    "        # Return default values in case of an error\n",
    "        return {\n",
    "            'Author_ID': 'Unknown',\n",
    "            'Author_Username': 'Unknown',\n",
    "            'Author_Discriminator': 'Unknown',\n",
    "            'Author_Global_Name': 'Unknown',\n",
    "            'Author_Primary_Guild': 'Unknown'\n",
    "        }\n",
    "\n",
    "# Apply the function to the Discord dataset and expand into separate columns\n",
    "author_info = discord_data['author'].apply(extract_author_info).apply(pd.Series)\n",
    "\n",
    "# Add the extracted fields to the main dataset\n",
    "discord_data = pd.concat([discord_data.drop(columns=['author']), author_info], axis=1)\n",
    "\n",
    "# Preview the cleaned Discord dataset\n",
    "print(discord_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee92160-71ad-4bd9-a2c6-f53fdb464f0b",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52a6b444-b55c-41b5-8d6e-cf2ffb5853dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  *The mother of learning, is repetition.*\\n*The...   \n",
      "1                                                ðŸ¤¨ ?   \n",
      "2  Only two things can do that. <a:FP_StarSparkle...   \n",
      "3  Fair enough. Will this take me from \"zero to h...   \n",
      "4                           It's your time to waste.   \n",
      "\n",
      "                          timestamp Author_Global_Name  \n",
      "0  2024-12-30T17:32:53.427000+00:00           The Blue  \n",
      "1  2024-12-30T17:32:10.251000+00:00          Wolverine  \n",
      "2  2024-12-30T17:31:35.458000+00:00           The Blue  \n",
      "3  2024-12-30T17:30:50.323000+00:00          Wolverine  \n",
      "4  2024-12-30T17:30:03.453000+00:00           The Blue  \n"
     ]
    }
   ],
   "source": [
    "# Drop the unnecessary columns\n",
    "discord_data = discord_data.drop(columns=['Author_ID', 'Author_Username', 'Author_Discriminator', 'Author_Primary_Guild'])\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(discord_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5da6a1-073c-4603-94ee-3b5f6e498fe2",
   "metadata": {},
   "source": [
    "# Reddit Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3257ba5-008b-4749-b978-a0a0ca48ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title     Content  Upvotes  \\\n",
      "0  Cisco Confirms Authenticity of Data After Seco...  No Content        2   \n",
      "1  Mining old data from NASAâ€™s Voyager 2 solves s...  No Content       14   \n",
      "2  Israel built an â€˜AI factoryâ€™ for war. It unlea...  No Content       33   \n",
      "3  Developer fires entire team for AI, now ends u...  No Content      629   \n",
      "4  FDA Approves First Generic of Once-Daily GLP-1...  No Content      157   \n",
      "\n",
      "   Comments_Count                                       Top_Comments  \\\n",
      "0               1  ['Damn ATT, Verizon, Cisco, social security, a...   \n",
      "1               3  ['Â« NASAâ€™s Voyager 2 flyby of Uranus decades a...   \n",
      "2               6  ['AI in war raises concerns.', 'When it comes ...   \n",
      "3              68  ['Creativity and problem-solving abilities, de...   \n",
      "4               9  ['This is generic victoza, which has been out ...   \n",
      "\n",
      "               Created              Author  \n",
      "0  2024-12-30 23:15:22          lurker_bee  \n",
      "1  2024-12-30 21:23:56              fchung  \n",
      "2  2024-12-30 20:23:18         MetaKnowing  \n",
      "3  2024-12-30 19:54:32  No-Information6622  \n",
      "4  2024-12-30 18:41:36          Peter55667  \n"
     ]
    }
   ],
   "source": [
    "# Drop the 'URL' column from the Reddit dataset\n",
    "reddit_data = reddit_data.drop(columns=['URL'])\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(reddit_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c55e7007-98cb-4a1a-bc4b-9e30ec26de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  Upvotes  Comments_Count  \\\n",
      "0  Cisco Confirms Authenticity of Data After Seco...        2               1   \n",
      "1  Mining old data from NASAâ€™s Voyager 2 solves s...       14               3   \n",
      "2  Israel built an â€˜AI factoryâ€™ for war. It unlea...       33               6   \n",
      "3  Developer fires entire team for AI, now ends u...      629              68   \n",
      "4  FDA Approves First Generic of Once-Daily GLP-1...      157               9   \n",
      "\n",
      "                                        Top_Comments              Created  \\\n",
      "0  ['Damn ATT, Verizon, Cisco, social security, a...  2024-12-30 23:15:22   \n",
      "1  ['Â« NASAâ€™s Voyager 2 flyby of Uranus decades a...  2024-12-30 21:23:56   \n",
      "2  ['AI in war raises concerns.', 'When it comes ...  2024-12-30 20:23:18   \n",
      "3  ['Creativity and problem-solving abilities, de...  2024-12-30 19:54:32   \n",
      "4  ['This is generic victoza, which has been out ...  2024-12-30 18:41:36   \n",
      "\n",
      "               Author                                      Title_Content  \n",
      "0          lurker_bee  Cisco Confirms Authenticity of Data After Seco...  \n",
      "1              fchung  Mining old data from NASAâ€™s Voyager 2 solves s...  \n",
      "2         MetaKnowing  Israel built an â€˜AI factoryâ€™ for war. It unlea...  \n",
      "3  No-Information6622  Developer fires entire team for AI, now ends u...  \n",
      "4          Peter55667  FDA Approves First Generic of Once-Daily GLP-1...  \n"
     ]
    }
   ],
   "source": [
    "# Merge Title and Content columns into a single column\n",
    "reddit_data['Title_Content'] = reddit_data['Title'] + \" \" + reddit_data['Content']\n",
    "\n",
    "# Drop the original Content column\n",
    "reddit_data = reddit_data.drop(columns=['Content'])\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(reddit_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13820a62-351d-4033-ad4f-c65d17bec79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "Title             0\n",
      "Upvotes           0\n",
      "Comments_Count    0\n",
      "Top_Comments      0\n",
      "Created           0\n",
      "Author            0\n",
      "Title_Content     0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 649 entries, 0 to 648\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Title           649 non-null    object\n",
      " 1   Upvotes         649 non-null    int64 \n",
      " 2   Comments_Count  649 non-null    int64 \n",
      " 3   Top_Comments    649 non-null    object\n",
      " 4   Created         649 non-null    object\n",
      " 5   Author          649 non-null    object\n",
      " 6   Title_Content   649 non-null    object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 35.6+ KB\n",
      "None\n",
      "                                               Title  Upvotes  Comments_Count  \\\n",
      "0  Cisco Confirms Authenticity of Data After Seco...        2               1   \n",
      "1  Mining old data from NASAâ€™s Voyager 2 solves s...       14               3   \n",
      "2  Israel built an â€˜AI factoryâ€™ for war. It unlea...       33               6   \n",
      "3  Developer fires entire team for AI, now ends u...      629              68   \n",
      "4  FDA Approves First Generic of Once-Daily GLP-1...      157               9   \n",
      "\n",
      "                                        Top_Comments              Created  \\\n",
      "0  ['Damn ATT, Verizon, Cisco, social security, a...  2024-12-30 23:15:22   \n",
      "1  ['Â« NASAâ€™s Voyager 2 flyby of Uranus decades a...  2024-12-30 21:23:56   \n",
      "2  ['AI in war raises concerns.', 'When it comes ...  2024-12-30 20:23:18   \n",
      "3  ['Creativity and problem-solving abilities, de...  2024-12-30 19:54:32   \n",
      "4  ['This is generic victoza, which has been out ...  2024-12-30 18:41:36   \n",
      "\n",
      "               Author                                      Title_Content  \n",
      "0          lurker_bee  Cisco Confirms Authenticity of Data After Seco...  \n",
      "1              fchung  Mining old data from NASAâ€™s Voyager 2 solves s...  \n",
      "2         MetaKnowing  Israel built an â€˜AI factoryâ€™ for war. It unlea...  \n",
      "3  No-Information6622  Developer fires entire team for AI, now ends u...  \n",
      "4          Peter55667  FDA Approves First Generic of Once-Daily GLP-1...  \n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(reddit_data.isnull().sum())\n",
    "\n",
    "# Preview the dataset to identify inconsistencies\n",
    "print(reddit_data.info())\n",
    "print(reddit_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd41739f-9d0a-4d1e-bed4-83964597b053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing duplicates: 649\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows\n",
    "reddit_data = reddit_data.drop_duplicates()\n",
    "\n",
    "# Confirm no duplicates remain\n",
    "print(f\"Number of rows after removing duplicates: {reddit_data.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecc274da-94c0-43c5-b4dc-dbd163640e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column after deduplication:\n",
      "Title             0\n",
      "Upvotes           0\n",
      "Comments_Count    0\n",
      "Top_Comments      0\n",
      "Created           0\n",
      "Author            0\n",
      "Title_Content     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "print(\"Missing values per column after deduplication:\")\n",
    "print(reddit_data.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values in essential columns\n",
    "reddit_data = reddit_data.dropna(subset=['Title', 'Created', 'Upvotes', 'Comments_Count'])\n",
    "\n",
    "# Fill missing values in other columns with defaults (if applicable)\n",
    "reddit_data['Top_Comments'] = reddit_data['Top_Comments'].fillna(\"[]\")  # Default to empty list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df530f94-d7bd-4635-beb7-0524b7c13935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Engagement Rate column\n",
    "reddit_data['Engagement_Rate'] = (reddit_data['Upvotes'] + reddit_data['Comments_Count']) / (\n",
    "    1 + reddit_data.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d88e9b-785a-4d04-a4f2-2efcb3855934",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_upvotes = reddit_data['Upvotes'].mean()\n",
    "avg_comments = reddit_data['Comments_Count'].mean()\n",
    "print(f\"Average Upvotes: {avg_upvotes}, Average Comments: {avg_comments}\")\n",
    "total_posts = reddit_data.shape[0]\n",
    "reddit_data['Engagement_Ratio'] = (reddit_data['Upvotes'] + reddit_data['Comments_Count']) / total_posts\n",
    "print(reddit_data[['Title', 'Engagement_Ratio']].head())\n",
    "reddit_data['Comment_to_Upvote_Ratio'] = reddit_data['Comments_Count'] / (reddit_data['Upvotes'] + 1e-9)\n",
    "print(reddit_data[['Title', 'Comment_to_Upvote_Ratio']].head())\n",
    "from datetime import datetime\n",
    "# Convert 'Created' to datetime\n",
    "reddit_data['Created'] = pd.to_datetime(reddit_data['Created'])\n",
    "reddit_data['Post_Lifetime_Days'] = (datetime.now() - reddit_data['Created']).dt.days\n",
    "print(reddit_data[['Title', 'Post_Lifetime_Days']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e119200a-fca8-4a1a-ae14-e7884c8a1517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Top Comments:\n",
      "0    ['Damn ATT, Verizon, Cisco, social security, a...\n",
      "1    ['Â« NASAâ€™s Voyager 2 flyby of Uranus decades a...\n",
      "2    ['AI in war raises concerns.', 'When it comes ...\n",
      "3    ['Creativity and problem-solving abilities, de...\n",
      "4    ['This is generic victoza, which has been out ...\n",
      "Name: Top_Comments, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary whitespace in Title\n",
    "reddit_data['Title'] = reddit_data['Title'].str.strip()\n",
    "\n",
    "# Preview cleaned Top_Comments\n",
    "print(\"Sample Top Comments:\")\n",
    "print(reddit_data['Top_Comments'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2daff-1788-4d47-b10c-fabb2e490b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c69f9dbd-cc9f-4821-9ca2-f6237543eb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved as 'cleaned_reddit_data.csv'\n"
     ]
    }
   ],
   "source": [
    "reddit_data.to_csv(\"cleaned_reddit_data.csv\", index=False)\n",
    "print(\"Cleaned dataset saved as 'cleaned_reddit_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62d59016-c87e-4828-b710-9e89b21c328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data['Source'] = 'Reddit'\n",
    "discord_data['Source'] = 'Discord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6df59409-bb85-4820-97e1-c150584009ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_renamed = reddit_data.rename(columns={\n",
    "    'Title': 'Topic',\n",
    "    'Created': 'Timestamp',\n",
    "    'Top_Comments': 'Text'\n",
    "})\n",
    "\n",
    "discord_data_renamed = discord_data.rename(columns={\n",
    "    'content': 'Text',\n",
    "    'timestamp': 'Timestamp',\n",
    "    'Author_Global_Name': 'Author'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "535010c4-5033-4f17-933e-c19922f1cb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text            Timestamp  \\\n",
      "0  ['Damn ATT, Verizon, Cisco, social security, a...  2024-12-30 23:15:22   \n",
      "1  ['Â« NASAâ€™s Voyager 2 flyby of Uranus decades a...  2024-12-30 21:23:56   \n",
      "2  ['AI in war raises concerns.', 'When it comes ...  2024-12-30 20:23:18   \n",
      "3  ['Creativity and problem-solving abilities, de...  2024-12-30 19:54:32   \n",
      "4  ['This is generic victoza, which has been out ...  2024-12-30 18:41:36   \n",
      "\n",
      "               Author  Source  \\\n",
      "0          lurker_bee  Reddit   \n",
      "1              fchung  Reddit   \n",
      "2         MetaKnowing  Reddit   \n",
      "3  No-Information6622  Reddit   \n",
      "4          Peter55667  Reddit   \n",
      "\n",
      "                                               Topic  \n",
      "0  Cisco Confirms Authenticity of Data After Seco...  \n",
      "1  Mining old data from NASAâ€™s Voyager 2 solves s...  \n",
      "2  Israel built an â€˜AI factoryâ€™ for war. It unlea...  \n",
      "3  Developer fires entire team for AI, now ends u...  \n",
      "4  FDA Approves First Generic of Once-Daily GLP-1...  \n"
     ]
    }
   ],
   "source": [
    "# Add a placeholder 'Topic' column to the Discord dataset\n",
    "discord_data_renamed['Topic'] = \"Unknown\"  # or None, based on your preference\n",
    "\n",
    "# Columns to keep for Reddit and Discord datasets\n",
    "reddit_columns_to_keep = ['Text', 'Timestamp', 'Author', 'Source', 'Topic']\n",
    "discord_columns_to_keep = ['Text', 'Timestamp', 'Author', 'Source', 'Topic']\n",
    "\n",
    "# Selecting relevant columns for both datasets\n",
    "reddit_data_final = reddit_data_renamed[reddit_columns_to_keep]\n",
    "discord_data_final = discord_data_renamed[discord_columns_to_keep]\n",
    "\n",
    "# Combine both datasets\n",
    "merged_data = pd.concat([reddit_data_final, discord_data_final], ignore_index=True)\n",
    "\n",
    "# Preview the merged dataset\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c7d3b56-aaa4-4565-b9d0-ff088849c7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset saved to 'merged_reddit_discord_data.csv'\n"
     ]
    }
   ],
   "source": [
    "merged_data.to_csv(\"merged_reddit_discord_data.csv\", index=False)\n",
    "print(\"Merged dataset saved to 'merged_reddit_discord_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
